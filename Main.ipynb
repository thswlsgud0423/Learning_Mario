{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import random\n",
    "import gym\n",
    "import gym_super_mario_bros\n",
    "from gym_super_mario_bros.actions import SIMPLE_MOVEMENT\n",
    "from nes_py.wrappers import JoypadSpace\n",
    "from gym.wrappers import RecordVideo\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import time "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use Deep Q-networks.\n",
    "\n",
    "The table would have 5×256^(84×84×4) since a state is a list of 4 contiguous 84x84 pixel frames, and we have 5 possible actions. \n",
    "-> This number is ridiculous so we have to resort to function approximation in which we use a nn to approximate the Q-table.\n",
    "\n",
    "Q*(s_t, a_t)←Q*(s_t, a_t) + α(r_t+1 + γmaxaQθ(s_{t+1}, a) - Q*(s_t, a_t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deep Q-Network\n",
    "class DQNSolver(nn.Module):\n",
    "    def __init__(self, input_shape, n_actions, memory_size=10000, batch_size=32, gamma=0.99, lr=1e-4):\n",
    "        super(DQNSolver, self).__init__()\n",
    "\n",
    "        # Here we are using a simple CNN model since the task is pretty simple. When we start another project, \n",
    "        # we could improve this part by using a different model or a pretrained model.\n",
    "\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(input_shape[0], 32, kernel_size=8, stride=4),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        conv_out_size = self._get_conv_out(input_shape)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(conv_out_size, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, n_actions)\n",
    "        )\n",
    "        self.memory_size = memory_size\n",
    "        self.memory_sample_size = batch_size\n",
    "        self.gamma = gamma\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=lr)\n",
    "        self.loss_fn = nn.SmoothL1Loss()\n",
    "        self.to(self.device)\n",
    "\n",
    "        # Experience replay memory\n",
    "        # The shpae is (state, action, reward, next_state, done)\n",
    "        self.STATE_MEM = torch.zeros((memory_size, *input_shape)).to(self.device)\n",
    "        self.ACTION_MEM = torch.zeros((memory_size, 1)).to(self.device)\n",
    "        self.REWARD_MEM = torch.zeros((memory_size, 1)).to(self.device)\n",
    "        self.STATE2_MEM = torch.zeros((memory_size, *input_shape)).to(self.device)\n",
    "        self.DONE_MEM = torch.zeros((memory_size, 1)).to(self.device)\n",
    "        self.ending_position = 0\n",
    "        self.num_in_queue = 0\n",
    "\n",
    "    def _get_conv_out(self, shape):\n",
    "        o = self.conv(torch.zeros(1, *shape))\n",
    "        return int(np.prod(o.size()))\n",
    "\n",
    "    def forward(self, x):\n",
    "        conv_out = self.conv(x).view(x.size()[0], -1)\n",
    "        return self.fc(conv_out)\n",
    "\n",
    "    def remember(self, state, action, reward, state2, done):\n",
    "        self.STATE_MEM[self.ending_position] = state.float()\n",
    "        self.ACTION_MEM[self.ending_position] = torch.tensor(action, dtype=torch.float32)\n",
    "        self.REWARD_MEM[self.ending_position] = torch.tensor(reward, dtype=torch.float32)\n",
    "        self.STATE2_MEM[self.ending_position] = state2.float()\n",
    "        self.DONE_MEM[self.ending_position] = torch.tensor(done, dtype=torch.float32)\n",
    "        self.ending_position = (self.ending_position + 1) % self.memory_size\n",
    "        self.num_in_queue = min(self.num_in_queue + 1, self.memory_size)\n",
    "\n",
    "    # recall prevents overfitting to recent experiences - standard RL without reply memory learns from consecutive states only\n",
    "    # recall reuses experiences\n",
    "    def recall(self):\n",
    "        idx = random.sample(range(self.num_in_queue), self.memory_sample_size)\n",
    "        return (\n",
    "            self.STATE_MEM[idx].to(self.device),\n",
    "            self.ACTION_MEM[idx].to(self.device),\n",
    "            self.REWARD_MEM[idx].to(self.device),\n",
    "            self.STATE2_MEM[idx].to(self.device),\n",
    "            self.DONE_MEM[idx].to(self.device),\n",
    "        )\n",
    "\n",
    "    def experience_replay(self):\n",
    "        if self.memory_sample_size > self.num_in_queue:\n",
    "            return\n",
    "        STATE, ACTION, REWARD, STATE2, DONE = self.recall()\n",
    "        self.optimizer.zero_grad()\n",
    "\n",
    "        # Bellman equation Q(s,a)= r + γ maxQ(s',a')\n",
    "        target = REWARD + self.gamma * self(STATE2).max(1).values.unsqueeze(1) * (1 - DONE)\n",
    "        current = self(STATE).gather(1, ACTION.long())\n",
    "        loss = self.loss_fn(current, target)\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "    \n",
    "    def save_model(self, path=\"dqn_model.pth\"):\n",
    "        torch.save(self.state_dict(), path)\n",
    "    \n",
    "    def load_model(self, path=\"dqn_model.pth\"):\n",
    "        self.load_state_dict(torch.load(path))\n",
    "        self.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkipFrame(gym.Wrapper):\n",
    "    def __init__(self, env, skip=4):\n",
    "        super(SkipFrame, self).__init__(env)\n",
    "        self.skip = skip\n",
    "\n",
    "    def step(self, action):\n",
    "        total_reward = 0.0\n",
    "        for _ in range(self.skip):\n",
    "            obs, reward, done, trunc, info = self.env.step(action)\n",
    "            total_reward += reward\n",
    "            if done:\n",
    "                break\n",
    "        return obs, total_reward, done, trunc, info\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jimmy\\miniconda3\\Lib\\site-packages\\gym\\envs\\registration.py:555: UserWarning: \u001b[33mWARN: The environment SuperMarioBros-1-1-v0 is out of date. You should consider upgrading to version `v3`.\u001b[0m\n",
      "  logger.warn(\n",
      "c:\\Users\\jimmy\\miniconda3\\Lib\\site-packages\\gym\\envs\\registration.py:627: UserWarning: \u001b[33mWARN: The environment creator metadata doesn't include `render_modes`, contains: ['render.modes', 'video.frames_per_second']\u001b[0m\n",
      "  logger.warn(\n",
      "c:\\Users\\jimmy\\miniconda3\\Lib\\site-packages\\pyglet\\image\\codecs\\wic.py:289: UserWarning: [WinError -2147417850] 스레드 모드가 설정된 후에는 바꿀 수 없습니다\n",
      "  warnings.warn(str(err))\n",
      "c:\\Users\\jimmy\\miniconda3\\Lib\\site-packages\\gym\\wrappers\\monitoring\\video_recorder.py:59: UserWarning: \u001b[33mWARN: Disabling video recorder because environment <SkipFrame<ProcessFrame84<JoypadSpace<TimeLimit<OrderEnforcing<PassiveEnvChecker<EnvCompatibility<SuperMarioBrosEnv<SuperMarioBros-1-1-v0>>>>>>>>> was not initialized with any compatible video mode between `rgb_array` and `rgb_array_list`\u001b[0m\n",
      "  logger.warn(\n",
      "c:\\Users\\jimmy\\miniconda3\\Lib\\site-packages\\gym\\utils\\passive_env_checker.py:272: UserWarning: \u001b[33mWARN: No render modes was declared in the environment (env.metadata['render_modes'] is None or not defined), you may have trouble when calling `.render()`.\u001b[0m\n",
      "  logger.warn(\n",
      "c:\\Users\\jimmy\\miniconda3\\Lib\\site-packages\\gym\\utils\\passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1/50000, Reward: 172.7, Steps: 26\n",
      "Episode 2/50000, Reward: 1020.9000000000003, Steps: 114\n",
      "Episode 3/50000, Reward: 618.7, Steps: 48\n",
      "Episode 4/50000, Reward: 221.0, Steps: 16\n",
      "Episode 5/50000, Reward: 598.0999999999999, Steps: 64\n",
      "Episode 6/50000, Reward: 213.7, Steps: 21\n",
      "Episode 7/50000, Reward: 793.1000000000004, Steps: 93\n",
      "Episode 8/50000, Reward: 580.4, Steps: 58\n",
      "Episode 9/50000, Reward: 205.39999999999998, Steps: 27\n",
      "Episode 10/50000, Reward: 238.0, Steps: 16\n",
      "Episode 11/50000, Reward: 169.7, Steps: 39\n",
      "Episode 12/50000, Reward: 202.7, Steps: 24\n",
      "Episode 13/50000, Reward: 221.0, Steps: 19\n",
      "Episode 14/50000, Reward: 237.1, Steps: 22\n",
      "Episode 15/50000, Reward: 197.7, Steps: 29\n",
      "Episode 16/50000, Reward: 242.0, Steps: 13\n",
      "Episode 17/50000, Reward: 245.0, Steps: 14\n",
      "Episode 18/50000, Reward: 998.2000000000005, Steps: 129\n",
      "Episode 19/50000, Reward: 247.0, Steps: 13\n",
      "Episode 20/50000, Reward: 209.0, Steps: 18\n",
      "Episode 21/50000, Reward: 597.3000000000003, Steps: 91\n",
      "Episode 22/50000, Reward: 565.8000000000002, Steps: 65\n",
      "Episode 23/50000, Reward: 205.0, Steps: 20\n",
      "Episode 24/50000, Reward: 618.0, Steps: 38\n",
      "Episode 25/50000, Reward: 223.0, Steps: 18\n",
      "Episode 26/50000, Reward: 1002.7000000000007, Steps: 118\n",
      "Episode 27/50000, Reward: 214.0, Steps: 20\n",
      "Episode 28/50000, Reward: 209.09999999999997, Steps: 19\n",
      "Episode 29/50000, Reward: 241.7, Steps: 19\n",
      "Episode 30/50000, Reward: 581.3000000000002, Steps: 81\n",
      "Episode 31/50000, Reward: 211.4, Steps: 36\n",
      "Episode 32/50000, Reward: 1075.2000000000014, Steps: 174\n",
      "Episode 33/50000, Reward: 185.39999999999998, Steps: 30\n",
      "Episode 34/50000, Reward: 238.0, Steps: 17\n",
      "Episode 35/50000, Reward: 237.0, Steps: 18\n",
      "Episode 36/50000, Reward: 201.79999999999995, Steps: 22\n",
      "Episode 37/50000, Reward: 752.7000000000007, Steps: 91\n",
      "Episode 38/50000, Reward: 600.8, Steps: 55\n",
      "Episode 39/50000, Reward: 591.1000000000001, Steps: 49\n",
      "Episode 40/50000, Reward: 187.79999999999995, Steps: 29\n",
      "Episode 41/50000, Reward: 208.0, Steps: 20\n",
      "Episode 42/50000, Reward: 219.7, Steps: 21\n",
      "Episode 43/50000, Reward: 198.0, Steps: 21\n",
      "Episode 44/50000, Reward: 1296.0000000000005, Steps: 101\n",
      "Episode 45/50000, Reward: 212.7, Steps: 20\n",
      "Episode 46/50000, Reward: 242.0, Steps: 14\n",
      "Episode 47/50000, Reward: 185.0, Steps: 22\n",
      "Episode 48/50000, Reward: 201.39999999999998, Steps: 20\n",
      "Episode 49/50000, Reward: 626.0, Steps: 50\n",
      "Episode 50/50000, Reward: 189.0, Steps: 33\n",
      "Episode 51/50000, Reward: 569.9000000000002, Steps: 89\n",
      "Episode 52/50000, Reward: 222.4, Steps: 26\n",
      "Episode 53/50000, Reward: 236.0, Steps: 16\n",
      "Episode 54/50000, Reward: 1010.7000000000005, Steps: 122\n",
      "Episode 55/50000, Reward: 244.0, Steps: 16\n",
      "Episode 56/50000, Reward: 1077.6000000000008, Steps: 195\n",
      "Episode 57/50000, Reward: 605.4000000000001, Steps: 47\n",
      "Episode 58/50000, Reward: 1292.7000000000007, Steps: 114\n",
      "Episode 59/50000, Reward: 181.7, Steps: 23\n",
      "Episode 60/50000, Reward: 725.5000000000005, Steps: 95\n",
      "Episode 61/50000, Reward: 794.2000000000003, Steps: 95\n",
      "Episode 62/50000, Reward: 761.3000000000015, Steps: 142\n",
      "Episode 63/50000, Reward: 241.0, Steps: 19\n",
      "Episode 64/50000, Reward: 619.800000000001, Steps: 81\n",
      "Episode 65/50000, Reward: 224.7, Steps: 21\n",
      "Episode 66/50000, Reward: 742.4000000000004, Steps: 92\n",
      "Episode 67/50000, Reward: 749.1000000000004, Steps: 103\n",
      "Episode 68/50000, Reward: 166.7, Steps: 27\n",
      "Episode 69/50000, Reward: 743.5000000000001, Steps: 62\n",
      "Episode 70/50000, Reward: 703.6000000000004, Steps: 77\n",
      "Episode 71/50000, Reward: 204.0, Steps: 22\n",
      "Episode 72/50000, Reward: 246.0, Steps: 14\n",
      "Episode 73/50000, Reward: 1139.2000000000003, Steps: 87\n",
      "Episode 74/50000, Reward: 211.7, Steps: 21\n",
      "Episode 75/50000, Reward: 757.8000000000004, Steps: 88\n",
      "Episode 76/50000, Reward: 216.0, Steps: 19\n",
      "Episode 77/50000, Reward: 964.1000000000015, Steps: 198\n",
      "Episode 78/50000, Reward: 232.0, Steps: 19\n",
      "Episode 79/50000, Reward: 228.0, Steps: 17\n",
      "Episode 80/50000, Reward: 1266.3000000000009, Steps: 186\n",
      "Episode 81/50000, Reward: 734.7000000000006, Steps: 113\n",
      "Episode 82/50000, Reward: 717.7000000000005, Steps: 85\n",
      "Episode 83/50000, Reward: 249.0, Steps: 13\n",
      "Episode 84/50000, Reward: 775.3000000000017, Steps: 114\n",
      "Episode 85/50000, Reward: 226.0, Steps: 17\n",
      "Episode 86/50000, Reward: 224.0, Steps: 19\n",
      "Episode 87/50000, Reward: 174.0, Steps: 29\n",
      "Episode 88/50000, Reward: 248.0, Steps: 14\n",
      "Episode 89/50000, Reward: 209.7, Steps: 26\n",
      "Episode 90/50000, Reward: 770.0, Steps: 70\n",
      "Episode 91/50000, Reward: 598.3000000000013, Steps: 121\n",
      "Episode 92/50000, Reward: 181.7, Steps: 29\n",
      "Episode 93/50000, Reward: 629.8000000000006, Steps: 67\n",
      "Episode 94/50000, Reward: 206.8, Steps: 38\n",
      "Episode 95/50000, Reward: 236.0, Steps: 18\n",
      "Episode 96/50000, Reward: 611.6000000000003, Steps: 71\n",
      "Episode 97/50000, Reward: 569.9000000000001, Steps: 74\n",
      "Episode 98/50000, Reward: 588.3, Steps: 60\n",
      "Episode 99/50000, Reward: 608.2, Steps: 75\n",
      "Episode 100/50000, Reward: 745.7000000000004, Steps: 78\n",
      "Episode 101/50000, Reward: 216.7, Steps: 20\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Frame processing wrapper\n",
    "class ProcessFrame84(gym.ObservationWrapper):\n",
    "    def __init__(self, env):\n",
    "        super(ProcessFrame84, self).__init__(env)\n",
    "        self.observation_space = gym.spaces.Box(low=0, high=255, shape=(1, 84, 84), dtype=np.uint8)\n",
    "\n",
    "    def observation(self, obs):\n",
    "        obs = cv2.cvtColor(obs, cv2.COLOR_RGB2GRAY)\n",
    "        obs = cv2.resize(obs, (84, 84), interpolation=cv2.INTER_AREA)\n",
    "        return np.expand_dims(obs, axis=0)  # Change axis to 0 to match PyTorch format\n",
    "\n",
    "\n",
    "# Environment Setup\n",
    "def make_env():\n",
    "    env = gym_super_mario_bros.make(\"SuperMarioBros-1-1-v0\", apply_api_compatibility=True, render_mode=\"human\")\n",
    "    env = JoypadSpace(env, SIMPLE_MOVEMENT)\n",
    "    env = ProcessFrame84(env)\n",
    "    env = SkipFrame(env, skip=10)  # Skips 4 frames per action\n",
    "    return env\n",
    "\n",
    "# Q-Value Visualization\n",
    "def plot_q_values(model, state):\n",
    "    with torch.no_grad():\n",
    "        q_values = model(torch.tensor(state).unsqueeze(0).float()).cpu().numpy()\n",
    "        sns.heatmap(q_values, annot=True, cmap=\"coolwarm\")\n",
    "        plt.xlabel(\"Actions\")\n",
    "        plt.ylabel(\"Q-Value\")\n",
    "        plt.show()\n",
    "\n",
    "# Training Loop\n",
    "if __name__ == \"__main__\":\n",
    "    env = make_env()\n",
    "    env = RecordVideo(env, \"./video\", episode_trigger=lambda e: e % 10 == 0)\n",
    "    writer = SummaryWriter()\n",
    "    input_shape = (1, 84, 84)\n",
    "    n_actions = env.action_space.n\n",
    "    model = DQNSolver(input_shape, n_actions)\n",
    "\n",
    "    # Initialize TensorBoard\n",
    "    writer = SummaryWriter(\"runs/mario_dqn\")\n",
    "\n",
    "    num_episodes = 50000\n",
    "    max_steps_per_episode = 10000\n",
    "    stuck_threshold = 60  # Reset if Mario is stuck\n",
    "    \n",
    "    gamma = 0.99  # Discount factor for future rewards\n",
    "    epsilon = 1.0  # Initial exploration rate\n",
    "    epsilon_min = 0.05  # Minimum exploration rate\n",
    "    epsilon_decay = 0.9995  # Decay rate\n",
    "\n",
    "    for episode in range(num_episodes):\n",
    "        state, _ = env.reset()\n",
    "        if not isinstance(state, torch.Tensor):\n",
    "            state = torch.tensor(state, dtype=torch.float32)\n",
    "\n",
    "        state = state.unsqueeze(0).squeeze(-1)\n",
    "\n",
    "        \n",
    "        total_reward = 0\n",
    "        steps = 0\n",
    "        stuck_counter = 0\n",
    "        last_position = 0\n",
    "        \n",
    "        while steps < max_steps_per_episode:\n",
    "            env.render()\n",
    "            time.sleep(0.000000001)\n",
    "\n",
    "            # ε-Greedy action selection\n",
    "            if random.random() < epsilon:\n",
    "                action = env.action_space.sample()  # Explore\n",
    "            else:\n",
    "                with torch.no_grad():\n",
    "        \n",
    "                    action = model(state).argmax().item()\n",
    "\n",
    "\n",
    "            next_state, reward, done, trunc, info = env.step(action)\n",
    "            next_state = torch.tensor(next_state, dtype=torch.float32).unsqueeze(0)\n",
    "\n",
    "            # If Mario is stuck\n",
    "            if info[\"x_pos\"] == last_position:\n",
    "                stuck_counter += 1\n",
    "                reward -= 0.3  # Penalty for being stuck bad boi\n",
    "            else:\n",
    "                stuck_counter = 0\n",
    "            last_position = info[\"x_pos\"]\n",
    "\n",
    "            model.remember(state, action, reward, next_state, done)\n",
    "            model.experience_replay()\n",
    "\n",
    "            total_reward += reward\n",
    "            state = next_state\n",
    "            steps += 1\n",
    "\n",
    "            if stuck_counter > stuck_threshold:\n",
    "                print(f\"Episode {episode + 1}: Mario got stuck! Resetting...\")\n",
    "                break\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "        \n",
    "        epsilon = max(epsilon_min, epsilon * epsilon_decay)\n",
    "\n",
    "        # Log progress\n",
    "        writer.add_scalar(\"Total Reward\", total_reward, episode)\n",
    "        writer.add_scalar(\"Steps\", steps, episode)\n",
    "\n",
    "        print(f\"Episode {episode + 1}/{num_episodes}, Reward: {total_reward}, Steps: {steps}\")\n",
    "\n",
    "        if (episode + 1) % 100 == 0:\n",
    "            model.save_model()\n",
    "\n",
    "    env.close()\n",
    "    writer.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def load_model(self, filename=\"mario_dqn.pth\"):\n",
    "    self.load_state_dict(torch.load(filename))\n",
    "    self.eval()\n",
    "    print(f\"Model loaded from {filename}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import gym_super_mario_bros\n",
    "from nes_py.wrappers import JoypadSpace\n",
    "from gym_super_mario_bros.actions import SIMPLE_MOVEMENT\n",
    "\n",
    "def demo():\n",
    "    env = gym_super_mario_bros.make(\"SuperMarioBros-1-1-v0\", apply_api_compatibility=True, render_mode=\"human\")\n",
    "    env = JoypadSpace(env, SIMPLE_MOVEMENT)\n",
    "    env = ProcessFrame84(env)\n",
    "\n",
    "    model = DQNSolver((1, 84, 84), env.action_space.n)\n",
    "    model.load_model(\"mario_dqn.pth\")\n",
    "\n",
    "    state, _ = env.reset()\n",
    "    state = torch.tensor(state, dtype=torch.float32).unsqueeze(0)\n",
    "\n",
    "    done = False\n",
    "    while not done:\n",
    "        env.render()\n",
    "        time.sleep(0.05)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            action = model(state).argmax().item()  # Select best action\n",
    "\n",
    "        next_state, _, done, trunc, _ = env.step(action)\n",
    "        state = torch.tensor(next_state, dtype=torch.float32).unsqueeze(0)\n",
    "\n",
    "    env.close()\n",
    "\n",
    "\n",
    "demo()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
